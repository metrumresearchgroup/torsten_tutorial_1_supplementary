
Bayesian inference offers a principled approach to learn about unknown variables from data using a probabilistic analysis.
The conclusions we draw are based on the posterior distribution which, in all but the simplest cases, is intractable.
We can however probe the posterior using a host of techniques such as Markov chains Monte Carlo sampling and approximate Bayesian computation.
Writing these algorithms is a tedious and error prone endeavor but fortunately modelers can often rely on existing software with efficient implementations.

In the field of pharmacometrics, statistical software such as NONMEM\textsuperscript{\textregistered} \cite{Beal_undated-qu} and Monolix\textsuperscript{\textregistered} \cite{monolix2021} support many routines to specify and analyze pharmacokinetic and pharmacodynamic population models.
There also exist more general probabilistic languages such as, to only name a few, BUGS \cite{lunn2009bugs} and more recently Stan \cite{Carpenter:2017}, which will be the focus of this tutorial.
Stan supports a rich library of probability densities, mathematical functions including matrix operations and numerical solvers for differential equations.
These features make for a rich and flexible language, however writing common pharmacometrics models can be tedious.
Torsten extends Stan by providing a suite of functions to facilitate the specification of pharmacometrics models.
These functions make it straightforward to model the event schedule of a clinical trial and parallelize computation across patients for population models.
% and also provide analytical solutions for ordinary differential equations (ODEs) which commonly arise in the field.


\subsection{Why Stan?}

We believe that Stan, coupled with Torsten, can be an important addition to the pharmacometrician's toolkit, especially for Bayesian data analysis.

The most obvious strength of Stan is its flexibility: it is straightforward to specify priors, systems of ODEs, a broad range of measurement models, missing data models and hierarchies (i.e. population models).
Because of this flexibility, various data sources and their corresponding measurement models can be combined into one large model, over which full Bayesian inference can be performed \cite[e.g][]{Weber:2018}.
There are not many examples in pharmacometrics where the flexibility of Stan would be fully utilized, but we believe this is in part because such tools were not readily available to pharmacometricians in the past.
The richness of the Stan language could well open the gate to new types of models.

Secondly, Stan supports state of the art inference algorithms, most notably its adaptive \textit{Hamiltonian Monte Carlo} sampler, a gradient-based Markov chains Monte Carlo algorithm \citep{Betancourt:2018} based on the No U-Turn sampler (NUTS) \cite{Hoffman:2014}, automatic differentiation variational inference (ADVI) \cite{Kucukelbir:2017}, and penalized maximum likelihood estimators.
Stan's inference algorithms are supported by a cutting edge automatic differentiation library, which efficiently generates the requisite derivatives \cite{Carpenter:2015}.
It is worth pointing out that algorithms, such as NUTS and ADVI, were first developed and implemented in Stan, before being widely adopted by the applied statistics and machine learning communities.
As of the writing of this article, new inference algorithms continue to be prototyped in Stan such as, to take a few recent examples, adjoint-differentiated Laplace approximations \cite{Margossian:2020}, cross-chain warmup \cite{Zhang:2020}, and path finding for improved chain initialization\footnote{Personal communication with the Stan development team.}.

Thirdly, Stan runs many \textit{many} diagnostics -- including the detection of divergent transitions \cite{Betancourt:2018}, and the improved computation of effective sample sizes and scale reduction factors, $\hat R$ \cite{Vehtari:2020}, and more -- and gives detailed warning messages.
This makes it considerably easier to identify issues with our inference and our models.
Several of these tools improve commonly used diagnostics which may not detect important problems, in which case our inference fails without us realizing it.
Stan fails better: it fails loudly. 

Last but not least, both Stan and Torsten are open-source projects, meaning  they are free and their source code can be examined and, if needed, scrutinized. 
The projects are under active development with new features being added regularly.
%Stan's vibrant community readily assists both new and experienced users, through the Stan forum: \url{https://discourse.mc-stan.org/}.

\subsection{Bayesian inference: notation, goals, and comments}

Given observed data, $\mathcal D$, and latent variables, $\theta$, a Bayesian model is defined by the joint distribution, $p(\mathcal D, \theta)$.
The latent variables can include model parameters, missing data, and more.
In this tutorial, we are mostly concerned with estimating model parameters.

The joint distribution observes a convenient decomposition,
\begin{equation*}
  p(\mathcal D, \theta) = p(\theta) p(\mathcal D \mid \theta),
\end{equation*}
%
with $p(\theta)$ the \textit{prior} distribution and $p(\mathcal D \mid \theta)$ the \textit{likelihood}.
The prior encodes information about the parameters, usually based on scientific expertise or results from previous analysis.
% It is a potent tool to encode prior knowledge -- hence the name -- or lack thereof using a probabilistic statement.
The likelihood tells us how the data is distributed for a fixed parameter value and, per one interpretation, can be thought of as a ``story of how the data is generated'' \cite{Gelman:2013b}.
%
The Bayesian proposition is to base our inference on the \textit{posterior} distribution, $p(\theta \mid \mathcal D)$.
% This posterior density describes how consistent various parameter values are with the model and the data.

%This framework requires us to treat the model parameters as \textit{random variables}.
%We here use the term to mean its measure theoretical definition and treat it as a purely mathematical object.
%This object then lends itself to various interpretations, which will be appropriate for different scenarios.
%In the context of Bayesian inference, we propose the following interpretation: we want to describe all the reasonable parameter values, quantify how consistent they are with our model and data, and to do this, the right mathematical formalism is that of a probability distribution.

For typical pharmacometric applications, the full joint posterior density of the model parameters is an unfathomable object which lives in a high dimensional space.
%It is worth pointing out that the posterior density is an unfathomable object which lives in a high dimensional %space.
%There is no such thing as ``computing the posterior distribution''.
Usually we cannot even numerically evaluate the posterior density at any particular point!
Instead we must probe the posterior distribution and learn the characteristics that interest us the most.
In our experience, this often includes a measure of a central tendency and a quantification of uncertainty, for example the mean and the variance, or the median and the $5^\mathrm{th}$ and $95^\mathrm{th}$ quantiles.
For skewed or multimodal distributions, we may want a more refined analysis which looks at many quantiles.
What we compute are estimates of these quantities. 
Most Bayesian inference involves calculations based on marginal posterior distributions. That typically requires integration over a high number of dimensions---an integration that is rarely tractable by analytic or numerical quadrature.
One strategy is to generate \textit{approximate} samples from the posterior distribution and then use sample mean, sample variance, and sample quantiles as our estimators.

Bayes' rule teaches us that
\begin{equation*}
  p(\theta \mid \mathcal D) = \frac{p(\mathcal D, \theta)}{p(\mathcal D)} =  \frac{p(\mathcal D \mid \theta) p(\theta)}{p(\mathcal D)}.
\end{equation*}
Typically we can evaluate the joint density in the nominator but not the normalizing constant, $p(\mathcal D)$, in the denominator.
%
A useful method must therefore be able to generate samples using the \textit{unnormalized} posterior density, $p(\mathcal D, \theta)$.
Many Markov chains Monte Carlo (MCMC) algorithms are designed to do exactly this.
Starting at an initial point, these chains explore the parameter space, $\Theta$, one iteration at a time, to produce the desired samples.
% A classic example is the Metropolis-Hastings algorithm, which generates samples using a random walk across $\Theta$ \cite{Metropolis:1953; Hastings:1970}.
% Unfortunately, this behavior scales poorly with the dimension of $\theta$, making it difficult to obtain representative samples.
% This behavior is exacerbated by the geometry of the posterior density that arises, for instance, in population models.
%
Hamiltonian Monte Carlo (HMC) is an MCMC method which uses the gradient to efficiently move across the parameter space \cite{Neal:2012, Betancourt:2018}.
Computationally, running HMC requires evaluating $\log p(\mathcal D, \theta)$ and $\nabla_\theta \log p(\mathcal D, \theta)$ many times across $\Theta$, i.e. for varying values of $\theta$ but fixed values of $\mathcal D$.
For this procedure to be well-defined, $\theta$ must be a continuous variable, else the requisite gradient does not exist.
Discrete parameters require a special treatment, which we will not discuss in this tutorial.

A Stan program specifies a method to evaluate $\log p(\mathcal D, \theta)$.
Thanks to automatic differentiation, this implicitly defines a procedure to compute $\nabla_\theta \log p(\mathcal D, \theta)$ \cite{Griewank:2008, Baydin:2018, Margossian:2019}.
Together, these two objects provide all the relevant information about our model to run HMC sampling and other gradient-based inference algorithms.

\subsection{Bayesian workflow}

Bayesian inference is only one step of a broader modeling process, which we might call the Bayesian workflow \cite{Betancourt:2018, Gabry:2019, Gelman:2020}.
Once we fit the model, we need to check the inference and if needed, fine tune our algorithm, or potentially change method.
And once we trust the inference, we naturally need to check the fitted model.
Our goal is to understand the shortcomings of our model and motivate useful revisions.
During the early stages of model development, this mostly comes down to troubleshooting our implementation and later this ``criticism'' step can lead to deeper insights.

All through the tutorial, we will demonstrate how Stan and Torsten can be used to check our inference and our fitted model.

\subsection{Setting up Stan and Torsten}

Detailed instructions on installing Stan and Torsten can be found on \url{https://github.com/metrumresearchgroup/Torsten}.
At its core, Stan is a C++ library but it can be interfaced with one of many scripting languages, including R, Python, and Julia.
We will use cmdStanR, which is a lightweight wrapper of Stan in R, and
% While RStan is compatible with Torsten and a popular option for R users, its development lags a little behind that of Stan.
in addition, the packages Posterior \cite{Bukner:2020} BayesPlot \cite{Gabry:2021}, and Loo \cite{Gabry:2020}.

The R and Stan code for all the examples are available at \url{link}.

 \subsection{Prerequisites and resources}
 
 Our aim is to provide a self-contained discussion, while trying to remain concise. 
The objectives of this tutorial are to describe basic model implementation and execution using Stan and Torsten and subsequent analysis of the results to generate MCMC diagnostics and limited model checking. Thus we illustrate key elements of a Bayesian workflow, but do not present complete Bayesian workflows for each example due to space limitations.
 We assume the reader is familiar with compartment models as they arise in pharmacokinetic and pharmacodynamic models, and has experience with data that describe a clinical event schedule.
 For the latter, we follow the convention established by NONMEM\textsuperscript{\textregistered}/NMTRAN\textsuperscript{\textregistered}/PREDPP\textsuperscript{\textregistered}.
 Exposure to Bayesian statistics and inference algorithms is desirable, in particular an elementary understanding of standard MCMC methods.
 We expect the reader to know R but we don't assume any background in Stan.
 
 Helpful reads include the \textit{Stan User Manual} \cite{Stan:2021} and the \textit{Torsten User Manual} \cite{Torsten:2021}. 
\textit{Statistical Rethinking} by McElreath (2020)\cite{mcelreath2020statistical} provides an excellent tutorial on Bayesian analysis that may be used for self-learning.
 A comprehensive textbook on Bayesian modeling is \textit{Bayesian Data Analysis} by Gelman et al (2013) \cite{Gelman:2013b}, with more recent insights on the Bayesian workflow provided by Gelman et al (2020) \cite{Gelman:2020}. 
 Betancourt (2018) offers an accessible discussion on MCMC methods, with an emphasis on HMC \cite{Betancourt:2018}.
 % We will discuss in the conclusion additional readings which can complement this tutorial.
   
 
 